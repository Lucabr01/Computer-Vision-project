{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4804bba-8f1e-4570-8546-6f255237c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U typing_extensions --cache-dir /workspace/pip_cache\n",
    "\n",
    "!pip show typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25721d37-430d-4859-a3bf-80c7d99278f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from torchvision import transforms\n",
    "from compressai.models import CompressionModel\n",
    "from compressai.entropy_models import EntropyBottleneck, GaussianConditional\n",
    "from compressai.layers import GDN\n",
    "\n",
    "import json\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision.models.optical_flow import raft_small, Raft_Small_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import flow_to_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf07477e-eb24-4f1a-90f6-ccd8476d0e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS and DATALOADER\n",
    "\n",
    "class VimeoSeptupletDataset(Dataset):\n",
    "    def __init__(self, root_dirs, split_files, transform=None):\n",
    "        self.transform = transform\n",
    "        self.sequences = []\n",
    "        self.sequence_to_root = {}\n",
    "        self.frames_per_sample = 5\n",
    "        self.samples_per_seq = 7 - self.frames_per_sample + 1\n",
    "        \n",
    "        if isinstance(root_dirs, str):\n",
    "            root_dirs = [root_dirs]\n",
    "        if isinstance(split_files, str):\n",
    "            split_files = [split_files]\n",
    "        \n",
    "        for root_dir, split_file in zip(root_dirs, split_files):\n",
    "            if not os.path.isabs(split_file):\n",
    "                split_file = os.path.join(root_dir, split_file)\n",
    "            \n",
    "            print(f\"Loading split from: {split_file}\")\n",
    "            \n",
    "            if os.path.exists(split_file):\n",
    "                with open(split_file, 'r') as f:\n",
    "                    count = 0\n",
    "                    for line in f:\n",
    "                        name = line.strip()\n",
    "                        if name:\n",
    "                            self.sequences.append(name)\n",
    "                            self.sequence_to_root[name] = root_dir\n",
    "                            count += 1\n",
    "                print(f\" Loaded {count} sequences\")\n",
    "            else:\n",
    "                print(f\" File not found {split_file}\")\n",
    "        \n",
    "        print(f\"Total Dataset: {len(self.sequences)} sequences\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences) * self.samples_per_seq\n",
    "    \n",
    "    def _get_sequence_path(self, seq_name, root_dir):\n",
    "        possible_paths = []\n",
    "        \n",
    "        if 'vimeo_settuplet_1' in root_dir or root_dir == \"\":\n",
    "            possible_paths.append(os.path.join(root_dir, \"vimeo_settuplet_1\", \"sequences\", seq_name))\n",
    "        elif 'vimeo_part2' in root_dir:\n",
    "            possible_paths.append(os.path.join(root_dir, \"vimeo_settuplet_2\", \"sequence\", seq_name))\n",
    "        else:\n",
    "            possible_paths.append(os.path.join(root_dir, \"sequence\", seq_name))\n",
    "        \n",
    "        possible_paths.extend([\n",
    "            os.path.join(root_dir, \"sequences\", seq_name),\n",
    "            os.path.join(root_dir, \"sequence\", seq_name),\n",
    "            os.path.join(root_dir, \"vimeo_settuplet_1\", \"sequences\", seq_name),\n",
    "            os.path.join(root_dir, \"vimeo_settuplet_2\", \"sequence\", seq_name),\n",
    "        ])\n",
    "        \n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "        \n",
    "        return possible_paths[0] if possible_paths else os.path.join(root_dir, \"sequence\", seq_name)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq_idx = idx // self.samples_per_seq\n",
    "        inner_idx = idx % self.samples_per_seq\n",
    "        seq_name = self.sequences[seq_idx]\n",
    "        root_dir = self.sequence_to_root[seq_name]\n",
    "        \n",
    "        seq_path = self._get_sequence_path(seq_name, root_dir)\n",
    "        \n",
    "        if not os.path.exists(seq_path):\n",
    "            raise FileNotFoundError(f\"Sequence path not found: {seq_path}\")\n",
    "        \n",
    "        start_frame = inner_idx + 1\n",
    "        frames = []\n",
    "        \n",
    "        for i in range(self.frames_per_sample):\n",
    "            frame_path = os.path.join(seq_path, f\"im{start_frame+i}.png\")\n",
    "            \n",
    "            if not os.path.exists(frame_path):\n",
    "                raise FileNotFoundError(f\"Frame not found: {frame_path}\")\n",
    "            \n",
    "            img = Image.open(frame_path).convert(\"RGB\")\n",
    "            \n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            else:\n",
    "                img = transforms.ToTensor()(img)\n",
    "            \n",
    "            frames.append(img)\n",
    "        \n",
    "        frame_curr = frames[-1]\n",
    "        frame_prev = frames[-2]\n",
    "        history = [frames[-2], frames[-3], frames[-4], frames[-5]]\n",
    "        history_frames = torch.cat(history, dim=0)\n",
    "        \n",
    "        return frame_curr, frame_prev, history_frames\n",
    "\n",
    "\n",
    "def conv(in_channels, out_channels, kernel_size=5, stride=2):\n",
    "    return nn.Conv2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=kernel_size,\n",
    "        stride=stride,\n",
    "        padding=kernel_size // 2,\n",
    "    )\n",
    "\n",
    "def deconv(in_channels, out_channels, kernel_size=5, stride=2):\n",
    "    return nn.ConvTranspose2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=kernel_size,\n",
    "        stride=stride,\n",
    "        output_padding=stride - 1,\n",
    "        padding=kernel_size // 2,\n",
    "    )\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, c, dilation=1, use_gn=True):\n",
    "        super().__init__()\n",
    "        self.use_gn = use_gn\n",
    "        self.conv1 = nn.Conv2d(c, c, 3, padding=dilation, dilation=dilation)\n",
    "        self.conv2 = nn.Conv2d(c, c, 3, padding=1)\n",
    "        if use_gn:\n",
    "            self.gn1 = nn.GroupNorm(8, c)\n",
    "            self.gn2 = nn.GroupNorm(8, c)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        if self.use_gn: y = self.gn1(y)\n",
    "        y = self.act(y)\n",
    "        y = self.conv2(y)\n",
    "        if self.use_gn: y = self.gn2(y)\n",
    "        return x + y\n",
    "\n",
    "class SimpleResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "# VAEs architectures:\n",
    "\n",
    "class ScaleHyperprior(CompressionModel):\n",
    "    def __init__(self, N, M, in_channels=2, out_channels=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.entropy_bottleneck = EntropyBottleneck(N)\n",
    "\n",
    "        self.g_a = nn.Sequential(\n",
    "            conv(in_channels, N),\n",
    "            GDN(N),\n",
    "            conv(N, N),\n",
    "            GDN(N),\n",
    "            conv(N, N),\n",
    "            GDN(N),\n",
    "            conv(N, M),\n",
    "        )\n",
    "\n",
    "        self.g_s = nn.Sequential(\n",
    "            deconv(M, N),\n",
    "            GDN(N, inverse=True),\n",
    "            deconv(N, N),\n",
    "            GDN(N, inverse=True),\n",
    "            deconv(N, N),\n",
    "            GDN(N, inverse=True),\n",
    "            deconv(N, out_channels),\n",
    "        )\n",
    "\n",
    "        self.h_a = nn.Sequential(\n",
    "            conv(M, N, stride=1, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv(N, N),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv(N, N),\n",
    "        )\n",
    "        \n",
    "        self.h_s = nn.Sequential(\n",
    "            deconv(N, N),\n",
    "            nn.ReLU(inplace=True),\n",
    "            deconv(N, N),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv(N, M, stride=1, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.gaussian_conditional = GaussianConditional(None)\n",
    "        self.N = int(N)\n",
    "        self.M = int(M)\n",
    "\n",
    "    @property\n",
    "    def downsampling_factor(self) -> int:\n",
    "        return 2 ** 4\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.g_a(x)\n",
    "        z = self.h_a(torch.abs(y))\n",
    "        z_hat, z_likelihoods = self.entropy_bottleneck(z)\n",
    "        scales_hat = self.h_s(z_hat)\n",
    "        y_hat, y_likelihoods = self.gaussian_conditional(y, scales_hat)\n",
    "        x_hat = self.g_s(y_hat)\n",
    "\n",
    "        return {\n",
    "            \"x_hat\": x_hat,\n",
    "            \"likelihoods\": {\"y\": y_likelihoods, \"z\": z_likelihoods},\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_state_dict(cls, state_dict):\n",
    "        \"\"\"Return a new model instance from `state_dict`.\"\"\"\n",
    "        N = state_dict[\"g_a.0.weight\"].size(0)\n",
    "        M = state_dict[\"g_a.6.weight\"].size(0)\n",
    "        in_channels = state_dict[\"g_a.0.weight\"].size(1) \n",
    "        out_channels = state_dict[\"g_s.6.weight\"].size(0)\n",
    "        \n",
    "        net = cls(N, M, in_channels=in_channels, out_channels=out_channels)\n",
    "        net.load_state_dict(state_dict)\n",
    "        return net\n",
    "\n",
    "    def compress(self, x):\n",
    "        y = self.g_a(x)\n",
    "        z = self.h_a(torch.abs(y))\n",
    "\n",
    "        z_strings = self.entropy_bottleneck.compress(z)\n",
    "        z_hat = self.entropy_bottleneck.decompress(z_strings, z.size()[-2:])\n",
    "\n",
    "        scales_hat = self.h_s(z_hat)\n",
    "        indexes = self.gaussian_conditional.build_indexes(scales_hat)\n",
    "        y_strings = self.gaussian_conditional.compress(y, indexes)\n",
    "        return {\"strings\": [y_strings, z_strings], \"shape\": z.size()[-2:]}\n",
    "\n",
    "    def decompress(self, strings, shape):\n",
    "        assert isinstance(strings, list) and len(strings) == 2\n",
    "        z_hat = self.entropy_bottleneck.decompress(strings[1], shape)\n",
    "        scales_hat = self.h_s(z_hat)\n",
    "        indexes = self.gaussian_conditional.build_indexes(scales_hat)\n",
    "        y_hat = self.gaussian_conditional.decompress(strings[0], indexes, z_hat.dtype)\n",
    "        \n",
    "        x_hat = self.g_s(y_hat) \n",
    "        \n",
    "        return {\"x_hat\": x_hat}\n",
    "\n",
    "# Flow postprocessing net\n",
    "\n",
    "class MotionRefineNET(nn.Module):\n",
    "    def __init__(self, base=64, blocks=8, use_gn=True, use_gate=True):\n",
    "        super().__init__()\n",
    "        in_ch = 14 \n",
    "        \n",
    "        self.stem = nn.Sequential(nn.Conv2d(in_ch, base, 3, padding=1), nn.ReLU(inplace=True))\n",
    "        \n",
    "        body = []\n",
    "        for i in range(blocks):\n",
    "            dil = 1 if i < blocks - 2 else 2\n",
    "            body.append(ResBlock(base, dilation=dil, use_gn=use_gn))\n",
    "        self.body = nn.Sequential(*body)\n",
    "\n",
    "        self.delta_head = nn.Conv2d(base, 2, 3, padding=1)\n",
    "        nn.init.zeros_(self.delta_head.weight)\n",
    "        nn.init.zeros_(self.delta_head.bias)\n",
    "\n",
    "        self.use_gate = use_gate\n",
    "        if use_gate:\n",
    "            self.gate_head = nn.Conv2d(base, 1, 3, padding=1)\n",
    "            nn.init.zeros_(self.gate_head.weight)\n",
    "            nn.init.zeros_(self.gate_head.bias)\n",
    "\n",
    "    def forward(self, flow_hat, history_4f):\n",
    "        x = torch.cat([flow_hat, history_4f], dim=1)\n",
    "        \n",
    "        f = self.stem(x)\n",
    "        f = self.body(f)\n",
    "        \n",
    "        delta = self.delta_head(f)\n",
    "        \n",
    "        if self.use_gate:\n",
    "            gate = torch.sigmoid(self.gate_head(f))\n",
    "            delta = gate * delta\n",
    "            \n",
    "        return flow_hat + delta\n",
    "\n",
    "# Residual postprocessing NET\n",
    "\n",
    "class ResRefiNET(nn.Module):\n",
    "    def __init__(self, in_channels=3, mid_channels=64, num_blocks=6):\n",
    "        super().__init__()\n",
    "        self.head = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1)\n",
    "        self.body = nn.Sequential(*[ResBlock(mid_channels, use_gn=True) for _ in range(num_blocks)])\n",
    "        self.tail = nn.Conv2d(mid_channels, in_channels, kernel_size=3, padding=1)\n",
    "        nn.init.zeros_(self.tail.weight)\n",
    "        nn.init.zeros_(self.tail.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.head(x)\n",
    "        out = self.body(out)\n",
    "        correction = self.tail(out)\n",
    "        return identity + correction\n",
    "\n",
    "# Frame recostruction NET\n",
    "\n",
    "class AdaptiveRefiNET(nn.Module):    \n",
    "    def __init__(self, base=64, num_blocks=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(19, base, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(base, base, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            SimpleResBlock(base) for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(base, base, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(base, 3, 3, 1, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, recon, warped, mask, history):\n",
    "        x = torch.cat([recon, warped, mask, history], dim=1)\n",
    "        \n",
    "        feat = self.encoder(x)\n",
    "        \n",
    "        for block in self.res_blocks:\n",
    "            feat = block(feat)\n",
    "        \n",
    "        correction = self.decoder(feat)\n",
    "        refined = recon + correction * mask\n",
    "        \n",
    "        return refined.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96369078-a9b9-4c04-a654-a4e83830d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "VIMEO_PARTS = [\n",
    "    \"\",\n",
    "    \"vimeo_part2\",\n",
    "    \"vimeo_part4\",\n",
    "    \"vimeo_part9\",\n",
    "]\n",
    "\n",
    "TRAIN_LISTS = [\n",
    "    \"vimeo_settuplet_1/sep_trainlist.txt\",\n",
    "    \"vimeo_settuplet_2/sep_trainlist.txt\",\n",
    "    \"sep_trainlist.txt\",\n",
    "    \"sep_trainlist.txt\",\n",
    "]\n",
    "\n",
    "TEST_LISTS = [\n",
    "    \"vimeo_settuplet_1/sep_testlist.txt\",\n",
    "    \"vimeo_settuplet_2/sep_testlist.txt\",\n",
    "    \"sep_testlist.txt\",\n",
    "    \"sep_testlist.txt\",\n",
    "]\n",
    "\n",
    "MVAE_CHECKPOINT = \"FlowVAE_finetune_ep11.pth\"\n",
    "MREFINE_CHECKPOINT = \"RefiFlow.pth\"\n",
    "RVAE_CHECKPOINT = \"ResidualVAE_HardMode_Ep4.pth\"\n",
    "RREFINE_CHECKPOINT = \"ResRefiNET.pth\"\n",
    "ADAPTIVE_CHECKPOINT = \"AdaptiveNET.pth\"  \n",
    "\n",
    "SAVE_DIR = \"checkpoints_adaptive\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10      \n",
    "START_EPOCH = 1       \n",
    "LR = 1e-4\n",
    "MAX_FLOW = 100.0\n",
    "\n",
    "\n",
    "\n",
    "def flow_warp(x, flow):\n",
    "    B, C, H, W = x.size()\n",
    "    xx = torch.arange(0, W).view(1, -1).repeat(H, 1)\n",
    "    yy = torch.arange(0, H).view(-1, 1).repeat(1, W)\n",
    "    xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "    yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "    grid = torch.cat((xx, yy), 1).float().to(x.device)\n",
    "    vgrid = grid + flow\n",
    "    vgrid[:, 0, :, :] = 2.0 * vgrid[:, 0, :, :] / max(W - 1, 1) - 1.0\n",
    "    vgrid[:, 1, :, :] = 2.0 * vgrid[:, 1, :, :] / max(H - 1, 1) - 1.0\n",
    "    vgrid = vgrid.permute(0, 2, 3, 1)\n",
    "    return F.grid_sample(x, vgrid, align_corners=True, mode='bilinear', padding_mode='border')\n",
    "\n",
    "\n",
    "def normalize_flow(flow, max_flow):\n",
    "    return flow.clamp(-max_flow, max_flow) / max_flow\n",
    "\n",
    "\n",
    "def compute_adaptive_mask(residual, lambda_param=1.5, epsilon=1e-6):\n",
    "    norm_per_pixel = torch.sqrt(torch.sum(residual ** 2, dim=1, keepdim=True))\n",
    "    H, W = residual.shape[2], residual.shape[3]\n",
    "    mu = torch.sum(norm_per_pixel, dim=(2, 3), keepdim=True) / (H * W)\n",
    "    mask = torch.tanh(lambda_param * norm_per_pixel / (mu + epsilon))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def fix_compressai_state_dict(state_dict):\n",
    "    new_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        new_key = key\n",
    "        if \"entropy_bottleneck.matrices.\" in key:\n",
    "            new_key = key.replace(\"matrices.\", \"_matrix\")\n",
    "        elif \"entropy_bottleneck.biases.\" in key:\n",
    "            new_key = key.replace(\"biases.\", \"_bias\")\n",
    "        elif \"entropy_bottleneck.factors.\" in key:\n",
    "            new_key = key.replace(\"factors.\", \"_factor\")\n",
    "        new_dict[new_key] = value\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(mvae, mrefinement, rvae, rrefinement, adaptive_net, raft, raft_transforms,\n",
    "                loader, optimizer, criterion, device):\n",
    "    adaptive_net.train()\n",
    "    total_loss = 0.0\n",
    "    total_mse_inter = 0.0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Train\", leave=False)\n",
    "    \n",
    "    for frame_curr, frame_prev, history_frames in pbar:\n",
    "        frame_curr = frame_curr.to(device, non_blocking=True)\n",
    "        frame_prev = frame_prev.to(device, non_blocking=True)\n",
    "        history_frames = history_frames.to(device, non_blocking=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Motion Branch\n",
    "            img1, img2 = raft_transforms(frame_prev, frame_curr)\n",
    "            flow_gt = raft(img1, img2)[-1]\n",
    "            flow_gt_norm = normalize_flow(flow_gt, MAX_FLOW)\n",
    "            \n",
    "            mvae_out = mvae(flow_gt_norm)\n",
    "            flow_coarse_norm = mvae_out[\"x_hat\"]\n",
    "            \n",
    "            flow_refined_norm = mrefinement(flow_coarse_norm, history_frames)\n",
    "            flow_refined_px = flow_refined_norm * MAX_FLOW\n",
    "            warped_image = flow_warp(frame_prev, flow_refined_px)\n",
    "            \n",
    "            # Residual Branch\n",
    "            residual_gt = frame_curr - warped_image\n",
    "            \n",
    "            rvae_out = rvae(residual_gt)\n",
    "            residual_coarse = rvae_out[\"x_hat\"]\n",
    "            \n",
    "            residual_refined = rrefinement(residual_coarse)\n",
    "            \n",
    "            # Intermediate reconstruction\n",
    "            recon_intermediate = (warped_image + residual_refined).clamp(0, 1)\n",
    "            mse_inter = criterion(recon_intermediate, frame_curr)\n",
    "            \n",
    "            # Compute mask\n",
    "            mask = compute_adaptive_mask(residual_refined)\n",
    "        \n",
    "        # Adaptive Refinement (TRAINABLE)\n",
    "        final_recon = adaptive_net(recon_intermediate, warped_image, mask, history_frames)\n",
    "        \n",
    "        # Loss\n",
    "        loss = criterion(final_recon, frame_curr)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(adaptive_net.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_mse_inter += mse_inter.item()\n",
    "        pbar.set_postfix({\"mse_final\": f\"{loss.item():.6f}\", \"mse_inter\": f\"{mse_inter.item():.6f}\"})\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_inter = total_mse_inter / len(loader)\n",
    "    return avg_loss, avg_inter\n",
    "\n",
    "\n",
    "def validate(mvae, mrefinement, rvae, rrefinement, adaptive_net, raft, raft_transforms,\n",
    "             loader, criterion, device):\n",
    "    adaptive_net.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mse_intermediate = 0.0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Val\", leave=False)\n",
    "    \n",
    "    for frame_curr, frame_prev, history_frames in pbar:\n",
    "        frame_curr = frame_curr.to(device)\n",
    "        frame_prev = frame_prev.to(device)\n",
    "        history_frames = history_frames.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Motion Branch\n",
    "            img1, img2 = raft_transforms(frame_prev, frame_curr)\n",
    "            flow_gt = raft(img1, img2)[-1]\n",
    "            flow_gt_norm = normalize_flow(flow_gt, MAX_FLOW)\n",
    "            \n",
    "            mvae_out = mvae(flow_gt_norm)\n",
    "            flow_coarse_norm = mvae_out[\"x_hat\"]\n",
    "            \n",
    "            flow_refined_norm = mrefinement(flow_coarse_norm, history_frames)\n",
    "            flow_refined_px = flow_refined_norm * MAX_FLOW\n",
    "            warped_image = flow_warp(frame_prev, flow_refined_px)\n",
    "            \n",
    "            # Residual Branch\n",
    "            residual_gt = frame_curr - warped_image\n",
    "            \n",
    "            rvae_out = rvae(residual_gt)\n",
    "            residual_coarse = rvae_out[\"x_hat\"]\n",
    "            \n",
    "            residual_refined = rrefinement(residual_coarse)\n",
    "            \n",
    "            # Intermediate reconstruction\n",
    "            recon_intermediate = (warped_image + residual_refined).clamp(0, 1)\n",
    "            mse_inter = criterion(recon_intermediate, frame_curr)\n",
    "            \n",
    "            # Compute mask\n",
    "            mask = compute_adaptive_mask(residual_refined)\n",
    "            \n",
    "            # Adaptive Refinement\n",
    "            final_recon = adaptive_net(recon_intermediate, warped_image, mask, history_frames)\n",
    "            \n",
    "            # Loss\n",
    "            loss = criterion(final_recon, frame_curr)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_mse_intermediate += mse_inter.item()\n",
    "    \n",
    "    avg_mse_final = total_loss / len(loader)\n",
    "    avg_mse_inter = total_mse_intermediate / len(loader)\n",
    "    \n",
    "    return avg_mse_final, avg_mse_inter\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    # Motion VAE\n",
    "    mvae = ScaleHyperprior(N=192, M=192, in_channels=2).to(DEVICE)\n",
    "    mvae.load_state_dict(torch.load(MVAE_CHECKPOINT, map_location=DEVICE, weights_only=False)[\"model_state_dict\"])\n",
    "    mvae.eval()\n",
    "    for p in mvae.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    # Motion Refinement\n",
    "    mrefinement = MotionRefineNET(base=64, blocks=8).to(DEVICE)\n",
    "    checkpoint = torch.load(MREFINE_CHECKPOINT, map_location=DEVICE, weights_only=False)\n",
    "    if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n",
    "        mrefinement.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    else:\n",
    "        mrefinement.load_state_dict(checkpoint)\n",
    "    mrefinement.eval()\n",
    "    for p in mrefinement.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    # Residual VAE\n",
    "    rvae = ScaleHyperprior(N=128, M=128, in_channels=3, out_channels=3).to(DEVICE)\n",
    "    checkpoint = torch.load(RVAE_CHECKPOINT, map_location=DEVICE, weights_only=False)\n",
    "    state_dict = checkpoint.get(\"model_state_dict\", checkpoint)\n",
    "    state_dict = fix_compressai_state_dict(state_dict)\n",
    "    rvae.load_state_dict(state_dict)\n",
    "    rvae.eval()\n",
    "    for p in rvae.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    # Residual Refinement\n",
    "    rrefinement = ResRefiNET().to(DEVICE)\n",
    "    checkpoint = torch.load(RREFINE_CHECKPOINT, map_location=DEVICE, weights_only=False)\n",
    "    if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n",
    "        rrefinement.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    else:\n",
    "        rrefinement.load_state_dict(checkpoint)\n",
    "    rrefinement.eval()\n",
    "    for p in rrefinement.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    # RAFT\n",
    "    from torchvision.models.optical_flow import raft_small, Raft_Small_Weights\n",
    "    raft = raft_small(weights=Raft_Small_Weights.DEFAULT, progress=False).to(DEVICE).eval()\n",
    "    raft_transforms = Raft_Small_Weights.DEFAULT.transforms()\n",
    "    \n",
    "    print(f\"\\nLoading AdaptiveRefiNET from checkpoint: {ADAPTIVE_CHECKPOINT}\")\n",
    "    adaptive_net = AdaptiveRefiNET(base=64, num_blocks=10).to(DEVICE)\n",
    "    \n",
    "    if os.path.exists(ADAPTIVE_CHECKPOINT):\n",
    "        checkpoint = torch.load(ADAPTIVE_CHECKPOINT, map_location=DEVICE, weights_only=False)\n",
    "        adaptive_net.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        \n",
    "        if \"epoch\" in checkpoint:\n",
    "            print(f\"Loaded from Epoch {checkpoint['epoch']}\")\n",
    "        if \"val_loss_final\" in checkpoint:\n",
    "            print(f\"Previous Val Loss: {checkpoint['val_loss_final']:.6f}\")\n",
    "        if \"val_improvement_pct\" in checkpoint or \"improvement_pct\" in checkpoint:\n",
    "            imp = checkpoint.get(\"val_improvement_pct\", checkpoint.get(\"improvement_pct\", \"N/A\"))\n",
    "            print(f\" Previous Improvement: {imp:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Checkpoint not found {ADAPTIVE_CHECKPOINT}\")\n",
    "        return\n",
    "    \n",
    "    print(\" AdaptiveRefiNET loaded (TRAINABLE)\")\n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in adaptive_net.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "\n",
    "    train_ds = VimeoSeptupletDataset(VIMEO_PARTS, TRAIN_LISTS, transforms.ToTensor())\n",
    "\n",
    "    val_ds = VimeoSeptupletDataset(VIMEO_PARTS, TEST_LISTS, transforms.ToTensor())\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                             num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                           num_workers=4, pin_memory=True)\n",
    "    \n",
    "    optimizer = optim.Adam(adaptive_net.parameters(), lr=LR)\n",
    "    \n",
    "    if \"optimizer_state_dict\" in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        print(\" Optimizer state restored\")\n",
    "    \n",
    "    remaining_epochs = NUM_EPOCHS - START_EPOCH + 1\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=remaining_epochs, eta_min=1e-6)\n",
    "    \n",
    "    criterion = nn.MSELoss().to(DEVICE)\n",
    "    \n",
    "    best_loss = checkpoint.get(\"val_loss_final\", float(\"inf\"))\n",
    "    print(f\" prev loss: {best_loss:.6f}\")\n",
    "    \n",
    "    metrics_file = os.path.join(SAVE_DIR, \"adaptive_metrics.json\")\n",
    "    if os.path.exists(metrics_file):\n",
    "        with open(metrics_file, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        print(f\" Loaded previous metrics ({len(metrics)} epochs)\")\n",
    "    else:\n",
    "        metrics = []\n",
    "    \n",
    "    \n",
    "    for epoch in range(START_EPOCH, NUM_EPOCHS + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{NUM_EPOCHS} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        \n",
    "        tr_loss, tr_inter = train_epoch(mvae, mrefinement, rvae, rrefinement, adaptive_net,\n",
    "                                        raft, raft_transforms, train_loader, optimizer, criterion, DEVICE)\n",
    "        \n",
    "        va_loss, va_inter = validate(mvae, mrefinement, rvae, rrefinement, adaptive_net,\n",
    "                                     raft, raft_transforms, val_loader, criterion, DEVICE)\n",
    "        \n",
    "        tr_improvement = ((tr_inter - tr_loss) / tr_inter * 100) if tr_inter > 0 else 0\n",
    "        va_improvement = ((va_inter - va_loss) / va_inter * 100) if va_inter > 0 else 0\n",
    "        \n",
    "        print(f\"Train  | MSE Inter: {tr_inter:.6f} | MSE Final: {tr_loss:.6f} | Improvement: {tr_improvement:.2f}%\")\n",
    "        print(f\"Val    | MSE Inter: {va_inter:.6f} | MSE Final: {va_loss:.6f} | Improvement: {va_improvement:.2f}%\")\n",
    "        \n",
    "        metrics.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_mse_intermediate\": tr_inter,\n",
    "            \"train_mse_final\": tr_loss,\n",
    "            \"train_improvement_pct\": tr_improvement,\n",
    "            \"val_mse_intermediate\": va_inter,\n",
    "            \"val_mse_final\": va_loss,\n",
    "            \"val_improvement_pct\": va_improvement\n",
    "        })\n",
    "        \n",
    "        if va_loss < best_loss:\n",
    "            best_loss = va_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': adaptive_net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss_final': va_loss,\n",
    "                'val_loss_intermediate': va_inter,\n",
    "                'improvement_pct': va_improvement\n",
    "            }, os.path.join(SAVE_DIR, \"adaptive_best.pth\"))\n",
    "            print(\" Best Model Saved\")\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': adaptive_net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss_final': va_loss,\n",
    "            'val_loss_intermediate': va_inter,\n",
    "        }, os.path.join(SAVE_DIR, \"adaptive_latest.pth\"))\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(\"finish\")\n",
    "    print(f\"Best Validation Loss: {best_loss:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f3afc2-3131-4d8c-8f1b-ecfe35a7e4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
