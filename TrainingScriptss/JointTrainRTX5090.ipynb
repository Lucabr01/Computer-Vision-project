{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd6a8a-7529-4803-8f6c-c373d99e78eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Disinstalla tutto\n",
    "!pip uninstall -y numpy scipy pandas compressai torch torchvision torchaudio\n",
    "\n",
    "#  Installa PyTorch per CUDA 12.1 (per 5090)\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "!pip install numpy==1.24.3 scipy==1.10.1\n",
    "\n",
    "!pip install compressai==1.2.4\n",
    "\n",
    "#  Restarta il kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65264a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  IGNORA\n",

    "!pip uninstall -y numpy scipy compressai torch torchvision\n",
    "\n",
    "#CompressAI requires specific numpy/scipy versions to function correctly.\n",
    "\n",
    "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install numpy==1.24.3\n",
    "!pip install scipy==1.10.1\n",
    "!pip install compressai==1.2.4\n",
    "\n",
    "# Restart the kernel !!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4804bba-8f1e-4570-8546-6f255237c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Aggiorna la libreria problematica\n",
    "!pip install -U typing_extensions --cache-dir /workspace/pip_cache\n",
    "\n",
    "# 2. Controllo opzionale (dovrebbe essere >= 4.10.0)\n",
    "!pip show typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf07477e-eb24-4f1a-90f6-ccd8476d0e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS and DATALOADER\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from torchvision import transforms\n",
    "from compressai.models import CompressionModel\n",
    "from compressai.entropy_models import EntropyBottleneck, GaussianConditional\n",
    "from compressai.layers import GDN\n",
    "\n",
    "import json\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision.models.optical_flow import raft_small, Raft_Small_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import flow_to_image\n",
    "\n",
    "\n",
    "class VimeoSeptupletDataset(Dataset):\n",
    "    def __init__(self, root_dirs, split_files, transform=None):\n",
    "        self.transform = transform\n",
    "        self.sequences = []\n",
    "        self.sequence_to_root = {}\n",
    "        self.frames_per_sample = 5 \n",
    "        self.samples_per_seq = 7 - self.frames_per_sample + 1\n",
    "        \n",
    "        if isinstance(root_dirs, str): \n",
    "            root_dirs = [root_dirs]\n",
    "        if isinstance(split_files, str): \n",
    "            split_files = [split_files]\n",
    "        \n",
    "        for root_dir, split_file in zip(root_dirs, split_files):\n",
    "            if not os.path.isabs(split_file):\n",
    "                split_file = os.path.join(root_dir, split_file)\n",
    "            \n",
    "            print(f\"Loading split from: {split_file}\")\n",
    "            \n",
    "            if os.path.exists(split_file):\n",
    "                with open(split_file, 'r') as f:\n",
    "                    count = 0\n",
    "                    for line in f:\n",
    "                        name = line.strip()\n",
    "                        if name:\n",
    "                            self.sequences.append(name)\n",
    "                            self.sequence_to_root[name] = root_dir\n",
    "                            count += 1\n",
    "                print(f\" Loaded {count} sequences\")\n",
    "            else:\n",
    "                print(f\"Error: File not found {split_file}\")\n",
    "        \n",
    "        print(f\" Total Dataset: {len(self.sequences)} sequences\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences) * self.samples_per_seq\n",
    "    \n",
    "    def _get_sequence_path(self, seq_name, root_dir):\n",
    "        possible_paths = []\n",
    "        \n",
    "        if 'vimeo_settuplet_1' in root_dir or root_dir == \"\":\n",
    "            possible_paths.append(os.path.join(root_dir, \"vimeo_settuplet_1\", \"sequences\", seq_name))\n",
    "        elif 'vimeo_part2' in root_dir:\n",
    "            possible_paths.append(os.path.join(root_dir, \"vimeo_settuplet_2\", \"sequence\", seq_name))\n",
    "        else:\n",
    "            possible_paths.append(os.path.join(root_dir, \"sequence\", seq_name))\n",
    "        \n",
    "        possible_paths.extend([\n",
    "            os.path.join(root_dir, \"sequences\", seq_name),\n",
    "            os.path.join(root_dir, \"sequence\", seq_name),\n",
    "            os.path.join(root_dir, \"vimeo_settuplet_1\", \"sequences\", seq_name),\n",
    "            os.path.join(root_dir, \"vimeo_settuplet_2\", \"sequence\", seq_name),\n",
    "        ])\n",
    "        \n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "        \n",
    "        return possible_paths[0] if possible_paths else os.path.join(root_dir, \"sequence\", seq_name)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq_idx = idx // self.samples_per_seq\n",
    "        inner_idx = idx % self.samples_per_seq\n",
    "        seq_name = self.sequences[seq_idx]\n",
    "        root_dir = self.sequence_to_root[seq_name]\n",
    "        \n",
    "        seq_path = self._get_sequence_path(seq_name, root_dir)\n",
    "        \n",
    "        if not os.path.exists(seq_path):\n",
    "             return torch.zeros(3, 256, 448), torch.zeros(3, 256, 448), torch.zeros(12, 256, 448)\n",
    "        \n",
    "        start_frame = inner_idx + 1\n",
    "        frames = []\n",
    "        \n",
    "        for i in range(self.frames_per_sample):\n",
    "            frame_path = os.path.join(seq_path, f\"im{start_frame+i}.png\")\n",
    "            \n",
    "            if not os.path.exists(frame_path):\n",
    "                img = Image.new('RGB', (448, 256))\n",
    "            else:\n",
    "                try:\n",
    "                    img = Image.open(frame_path).convert(\"RGB\")\n",
    "                except:\n",
    "                    img = Image.new('RGB', (448, 256))\n",
    "            \n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            else:\n",
    "                img = transforms.ToTensor()(img)\n",
    "            \n",
    "            frames.append(img)\n",
    "        \n",
    "        frame_curr = frames[-1]\n",
    "        frame_prev = frames[-2]\n",
    "        history = [frames[-2], frames[-3], frames[-4], frames[-5]]\n",
    "        history_frames = torch.cat(history, dim=0)\n",
    "        \n",
    "        return frame_curr, frame_prev, history_frames\n",
    "\n",
    "\n",
    "def conv(in_channels, out_channels, kernel_size=5, stride=2):\n",
    "    return nn.Conv2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=kernel_size,\n",
    "        stride=stride,\n",
    "        padding=kernel_size // 2,\n",
    "    )\n",
    "\n",
    "def deconv(in_channels, out_channels, kernel_size=5, stride=2):\n",
    "    return nn.ConvTranspose2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=kernel_size,\n",
    "        stride=stride,\n",
    "        output_padding=stride - 1,\n",
    "        padding=kernel_size // 2,\n",
    "    )\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, c, dilation=1, use_gn=True):\n",
    "        super().__init__()\n",
    "        self.use_gn = use_gn\n",
    "        self.conv1 = nn.Conv2d(c, c, 3, padding=dilation, dilation=dilation)\n",
    "        self.conv2 = nn.Conv2d(c, c, 3, padding=1)\n",
    "        if use_gn:\n",
    "            self.gn1 = nn.GroupNorm(8, c)\n",
    "            self.gn2 = nn.GroupNorm(8, c)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        if self.use_gn: y = self.gn1(y)\n",
    "        y = self.act(y)\n",
    "        y = self.conv2(y)\n",
    "        if self.use_gn: y = self.gn2(y)\n",
    "        return x + y\n",
    "\n",
    "class SimpleResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "# VAEs architectures:\n",
    "\n",
    "class ScaleHyperprior(CompressionModel):\n",
    "    def __init__(self, N, M, in_channels=2, out_channels=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.entropy_bottleneck = EntropyBottleneck(N)\n",
    "\n",
    "        self.g_a = nn.Sequential(\n",
    "            conv(in_channels, N),\n",
    "            GDN(N),\n",
    "            conv(N, N),\n",
    "            GDN(N),\n",
    "            conv(N, N),\n",
    "            GDN(N),\n",
    "            conv(N, M),\n",
    "        )\n",
    "\n",
    "        self.g_s = nn.Sequential(\n",
    "            deconv(M, N),\n",
    "            GDN(N, inverse=True),\n",
    "            deconv(N, N),\n",
    "            GDN(N, inverse=True),\n",
    "            deconv(N, N),\n",
    "            GDN(N, inverse=True),\n",
    "            deconv(N, out_channels),\n",
    "        )\n",
    "\n",
    "        self.h_a = nn.Sequential(\n",
    "            conv(M, N, stride=1, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv(N, N),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv(N, N),\n",
    "        )\n",
    "        \n",
    "        self.h_s = nn.Sequential(\n",
    "            deconv(N, N),\n",
    "            nn.ReLU(inplace=True),\n",
    "            deconv(N, N),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv(N, M, stride=1, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.gaussian_conditional = GaussianConditional(None)\n",
    "        self.N = int(N)\n",
    "        self.M = int(M)\n",
    "\n",
    "    @property\n",
    "    def downsampling_factor(self) -> int:\n",
    "        return 2 ** 4\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.g_a(x)\n",
    "        z = self.h_a(torch.abs(y))\n",
    "        z_hat, z_likelihoods = self.entropy_bottleneck(z)\n",
    "        scales_hat = self.h_s(z_hat)\n",
    "        y_hat, y_likelihoods = self.gaussian_conditional(y, scales_hat)\n",
    "        x_hat = self.g_s(y_hat)\n",
    "\n",
    "        return {\n",
    "            \"x_hat\": x_hat,\n",
    "            \"likelihoods\": {\"y\": y_likelihoods, \"z\": z_likelihoods},\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_state_dict(cls, state_dict):\n",
    "        \"\"\"Return a new model instance from `state_dict`.\"\"\"\n",
    "        N = state_dict[\"g_a.0.weight\"].size(0)\n",
    "        M = state_dict[\"g_a.6.weight\"].size(0)\n",
    "        in_channels = state_dict[\"g_a.0.weight\"].size(1) \n",
    "        out_channels = state_dict[\"g_s.6.weight\"].size(0)\n",
    "        \n",
    "        net = cls(N, M, in_channels=in_channels, out_channels=out_channels)\n",
    "        net.load_state_dict(state_dict)\n",
    "        return net\n",
    "\n",
    "    def compress(self, x):\n",
    "        y = self.g_a(x)\n",
    "        z = self.h_a(torch.abs(y))\n",
    "\n",
    "        z_strings = self.entropy_bottleneck.compress(z)\n",
    "        z_hat = self.entropy_bottleneck.decompress(z_strings, z.size()[-2:])\n",
    "\n",
    "        scales_hat = self.h_s(z_hat)\n",
    "        indexes = self.gaussian_conditional.build_indexes(scales_hat)\n",
    "        y_strings = self.gaussian_conditional.compress(y, indexes)\n",
    "        return {\"strings\": [y_strings, z_strings], \"shape\": z.size()[-2:]}\n",
    "\n",
    "    def decompress(self, strings, shape):\n",
    "        assert isinstance(strings, list) and len(strings) == 2\n",
    "        z_hat = self.entropy_bottleneck.decompress(strings[1], shape)\n",
    "        scales_hat = self.h_s(z_hat)\n",
    "        indexes = self.gaussian_conditional.build_indexes(scales_hat)\n",
    "        y_hat = self.gaussian_conditional.decompress(strings[0], indexes, z_hat.dtype)\n",
    "        \n",
    "        x_hat = self.g_s(y_hat) \n",
    "        \n",
    "        return {\"x_hat\": x_hat}\n",
    "\n",
    "# Flow postprocessing net\n",
    "\n",
    "class MotionRefineNET(nn.Module):\n",
    "    def __init__(self, base=64, blocks=8, use_gn=True, use_gate=True):\n",
    "        super().__init__()\n",
    "\n",
    "        in_ch = 14 \n",
    "        \n",
    "        self.stem = nn.Sequential(nn.Conv2d(in_ch, base, 3, padding=1), nn.ReLU(inplace=True))\n",
    "        \n",
    "        body = []\n",
    "        for i in range(blocks):\n",
    "            dil = 1 if i < blocks - 2 else 2\n",
    "            body.append(ResBlock(base, dilation=dil, use_gn=use_gn))\n",
    "        self.body = nn.Sequential(*body)\n",
    "\n",
    "        self.delta_head = nn.Conv2d(base, 2, 3, padding=1)\n",
    "        nn.init.zeros_(self.delta_head.weight)\n",
    "        nn.init.zeros_(self.delta_head.bias)\n",
    "\n",
    "        self.use_gate = use_gate\n",
    "        if use_gate:\n",
    "            self.gate_head = nn.Conv2d(base, 1, 3, padding=1)\n",
    "            nn.init.zeros_(self.gate_head.weight)\n",
    "            nn.init.zeros_(self.gate_head.bias)\n",
    "\n",
    "    def forward(self, flow_hat, history_4f):\n",
    "\n",
    "        x = torch.cat([flow_hat, history_4f], dim=1)\n",
    "        \n",
    "        f = self.stem(x)\n",
    "        f = self.body(f)\n",
    "        \n",
    "        delta = self.delta_head(f)\n",
    "        \n",
    "        if self.use_gate:\n",
    "            gate = torch.sigmoid(self.gate_head(f))\n",
    "            delta = gate * delta\n",
    "            \n",
    "        return flow_hat + delta\n",
    "\n",
    "# Residual postprocessing NET\n",
    "\n",
    "class ResRefiNET(nn.Module):\n",
    "    def __init__(self, in_channels=3, mid_channels=64, num_blocks=6):\n",
    "        super().__init__()\n",
    "        self.head = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1)\n",
    "        self.body = nn.Sequential(*[ResBlock(mid_channels, use_gn=True) for _ in range(num_blocks)])\n",
    "        self.tail = nn.Conv2d(mid_channels, in_channels, kernel_size=3, padding=1)\n",
    "        nn.init.zeros_(self.tail.weight)\n",
    "        nn.init.zeros_(self.tail.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.head(x)\n",
    "        out = self.body(out)\n",
    "        correction = self.tail(out)\n",
    "        return identity + correction\n",
    "\n",
    "# Frame recostruction NET\n",
    "\n",
    "class AdaptiveRefiNET(nn.Module):    \n",
    "    def __init__(self, base=64, num_blocks=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(19, base, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(base, base, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            SimpleResBlock(base) for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(base, base, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(base, 3, 3, 1, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, recon, warped, mask, history):\n",
    "        x = torch.cat([recon, warped, mask, history], dim=1)\n",
    "        \n",
    "        feat = self.encoder(x)\n",
    "        \n",
    "        for block in self.res_blocks:\n",
    "            feat = block(feat)\n",
    "        \n",
    "        correction = self.decoder(feat)\n",
    "        refined = recon + correction * mask\n",
    "        \n",
    "        return refined.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807abb4-5607-4986-8a73-a13215dd1049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qui il problema piu grande è stato far andare la 5090 senza crash di CUDA\n",
    "# RunPod quando si cambia da gpu x a gpu y fa casini con la cache :(\n",
    "# Senza almeno 32GB di VRAM il training sarebbe stato infattibile\n",
    "# Volevamo finetunare anche il Motion Estimator (RAFT) ma sarebbe stato troppo pesante e lento.\n",
    "# Avevamo provato con una A100 con 80GB di VRAM ma il settaggio era molto complicato e anche una volta runnato andava a 3 fps di inferenza.\n",
    "# Il bilancio costo/qualità non ne valeva la pena.\n",
    "\n",
    "# La vera sfida è stata la scelta degli iperparametri giusti di rete e training, poichè il tempo e le risorse per sbagliare e \n",
    "# ricominciare erano davvero limitate.\n",
    "\n",
    "\n",
    "\n",
    "# Gestione memoria ottimizzata per 32GB VRAM (GDDR7)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,roundup_power2_divisions:16\"\n",
    "\n",
    "# Evita riuso cache Inductor/Triton compilate per altre GPU (Runpod moment)\n",
    "os.environ.setdefault(\"TORCHINDUCTOR_CACHE_DIR\", \"/tmp/torchinductor_5090\")\n",
    "os.environ.setdefault(\"TRITON_CACHE_DIR\", \"/tmp/triton_5090\")\n",
    "\n",
    "# TF32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Valori per ottimizzare l'addestramento su RTX 5090 \n",
    "# BATCH SIZE MASSIMO A 16 CON GRAD_ACC A 2 SENNO CRASHA RUNPOD!!!!!!!!!!!!!!!\n",
    "# Cosi va a 3fps ma bisogna accettare il compromesso. RAFT è pesantissimo. \n",
    "\n",
    "BATCH_SIZE = 12\n",
    "GRADIENT_ACCUMULATION = 3\n",
    "\n",
    "NUM_WORKERS = 8\n",
    "PREFETCH_FACTOR = 4\n",
    "PIN_MEMORY = True\n",
    "PERSISTENT_WORKERS = True\n",
    "\n",
    "USE_BFLOAT16 = True\n",
    "AUTOCAST_DTYPE = torch.bfloat16 if USE_BFLOAT16 else torch.float16\n",
    "\n",
    "MAX_FLOW = 50.0\n",
    "AUX_WEIGHT = 0.0\n",
    "EXTRA_EPOCHS = 10\n",
    "\n",
    "\n",
    "USE_COMPILE = True\n",
    "COMPILE_BACKEND = \"aot_eager\"  # niente Triton/Inductor da problemi di compatibilità su RTX 5090\n",
    "\n",
    "# Qui volevamo implementare una schedulazione di lambda variabile per l'addestramento, ma ci abbiamo ripensato per semplicità della rete.\n",
    "\n",
    "LAMBDA_SCHEDULE = {\n",
    "    6:  1024.0,\n",
    "    7:  1024.0,\n",
    "    8:  1024.0,\n",
    "    9:  1024.0,\n",
    "    10: 1024.0,\n",
    "    11: 1024.0,\n",
    "    12: 1024.0,\n",
    "    13: 1024.0,\n",
    "    14: 1024.0,\n",
    "    15: 1024.0,\n",
    "}\n",
    "\n",
    "DEBUG_RECON_RANGE_EVERY = 200\n",
    "\n",
    "VIMEO_PARTS = [\n",
    "    \"\", \"vimeo_part2\", \"vimeo-part3\", \"vimeo_part4\",\n",
    "    \"vimeo_part5\", \"vimeo_part6\", \"vimeo-part7\",\n",
    "    \"vimeo-part8\", \"vimeo_part9\",\n",
    "]\n",
    "TRAIN_LISTS = [\n",
    "    \"vimeo_settuplet_1/sep_trainlist.txt\", \"vimeo_settuplet_2/sep_trainlist.txt\",\n",
    "    \"sep_trainlist.txt\", \"sep_trainlist.txt\", \"sep_trainlist.txt\",\n",
    "    \"sep_trainlist.txt\", \"sep_trainlist.txt\", \"sep_trainlist.txt\", \"sep_trainlist.txt\",\n",
    "]\n",
    "TEST_LISTS = [\n",
    "    \"vimeo_settuplet_1/sep_testlist.txt\", \"vimeo_settuplet_2/sep_testlist.txt\",\n",
    "    \"sep_testlist.txt\", \"sep_testlist.txt\", \"sep_testlist.txt\",\n",
    "    \"sep_testlist.txt\", \"sep_testlist.txt\", \"sep_testlist.txt\", \"sep_testlist.txt\",\n",
    "]\n",
    "\n",
    "OUTPUT_DIR = \"workspace/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "RESUME_PATH = os.path.join(\"joint_epoch_6.pth\")\n",
    "\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "class RateDistortionLossMSE(nn.Module):\n",
    "    def __init__(self, lmbda=1024.0):\n",
    "        super().__init__()\n",
    "        self.lmbda = float(lmbda)\n",
    "\n",
    "    def forward(self, recon, target, likelihoods_list):\n",
    "        recon_f = recon.float()\n",
    "        target_f = target.float()\n",
    "\n",
    "        mse = F.mse_loss(recon_f, target_f)\n",
    "\n",
    "        n, _, h, w = target_f.size()\n",
    "        num_pixels = n * h * w\n",
    "\n",
    "        rate = 0.0\n",
    "        for likelihoods in likelihoods_list:\n",
    "            if likelihoods is None:\n",
    "                continue\n",
    "            for key in [\"y\", \"z\"]:\n",
    "                if key in likelihoods:\n",
    "                    ll = torch.log(likelihoods[key] + 1e-10)\n",
    "                    rate += ll.sum() / (-math.log(2) * num_pixels)\n",
    "\n",
    "        loss = rate + self.lmbda * mse\n",
    "        return loss, mse, rate, mse, torch.tensor(0.0, device=recon.device)\n",
    "\n",
    "\n",
    "def unwrap(model: nn.Module) -> nn.Module:\n",
    "    return model._orig_mod if hasattr(model, \"_orig_mod\") else model\n",
    "\n",
    "\n",
    "def safe_update_entropy(model: nn.Module, force: bool = True):\n",
    "    m = unwrap(model)\n",
    "    if hasattr(m, \"update\"):\n",
    "        m.update(force=force)\n",
    "\n",
    "\n",
    "def safe_state_dict(model: nn.Module):\n",
    "    return unwrap(model).state_dict()\n",
    "\n",
    "\n",
    "def normalize_flow(flow, max_flow):\n",
    "    return flow.clamp(-max_flow, max_flow) / max_flow\n",
    "\n",
    "\n",
    "def flow_warp(x, flow):\n",
    "    B, C, H, W = x.size()\n",
    "\n",
    "    xx = torch.arange(0, W, device=x.device, dtype=x.dtype).view(1, -1).repeat(H, 1)\n",
    "    yy = torch.arange(0, H, device=x.device, dtype=x.dtype).view(-1, 1).repeat(1, W)\n",
    "    xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "    yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "    grid = torch.cat((xx, yy), 1)\n",
    "\n",
    "    vgrid = grid + flow\n",
    "    vgrid[:, 0] = 2.0 * vgrid[:, 0] / max(W - 1, 1) - 1.0\n",
    "    vgrid[:, 1] = 2.0 * vgrid[:, 1] / max(H - 1, 1) - 1.0\n",
    "    vgrid = vgrid.permute(0, 2, 3, 1)\n",
    "\n",
    "    return F.grid_sample(x, vgrid, align_corners=True, mode='bilinear', padding_mode='border')\n",
    "\n",
    "\n",
    "def compute_adaptive_mask(residual, lambda_param=1.5, epsilon=1e-6):\n",
    "    norm_per_pixel = torch.sqrt(torch.sum(residual ** 2, dim=1, keepdim=True) + epsilon)\n",
    "    H, W = residual.shape[2], residual.shape[3]\n",
    "    mu = torch.sum(norm_per_pixel, dim=(2, 3), keepdim=True) / (H * W)\n",
    "    mask = torch.tanh(lambda_param * norm_per_pixel / (mu + epsilon))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def total_aux_loss(models):\n",
    "    aux = 0.0\n",
    "    for k in ['mvae', 'rvae']:\n",
    "        m = unwrap(models[k])\n",
    "        if hasattr(m, \"aux_loss\"):\n",
    "            aux = aux + m.aux_loss()\n",
    "    return aux\n",
    "\n",
    "\n",
    "def freeze_entropy_modules(scale_hyperprior_model: nn.Module):\n",
    "    model = unwrap(scale_hyperprior_model)\n",
    "\n",
    "    frozen = 0\n",
    "    for attr in [\"entropy_bottleneck\", \"gaussian_conditional\"]:\n",
    "        if hasattr(model, attr):\n",
    "            mod = getattr(model, attr)\n",
    "            for p in mod.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.requires_grad = False\n",
    "                    frozen += p.numel()\n",
    "    return frozen\n",
    "\n",
    "\n",
    "def resume_from_checkpoint(path, models, optimizer=None, scheduler=None):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Resume checkpoint non trovato: {path}\")\n",
    "\n",
    "    print(f\"Loading checkpoint from {path}...\")\n",
    "    ckpt = torch.load(path, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "    def load_weights(model, state_dict):\n",
    "        try:\n",
    "            model.load_state_dict(state_dict, strict=True)\n",
    "        except RuntimeError:\n",
    "            new_state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}\n",
    "            model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "    load_weights(models['mvae'], ckpt['mvae'])\n",
    "    load_weights(models['mrefine'], ckpt['mrefine'])\n",
    "    load_weights(models['rvae'], ckpt['rvae'])\n",
    "    load_weights(models['rrefine'], ckpt['rrefine'])\n",
    "    load_weights(models['adaptive'], ckpt['adaptive'])\n",
    "\n",
    "    if optimizer is not None and 'optimizer' in ckpt:\n",
    "        optimizer.load_state_dict(ckpt['optimizer'])\n",
    "    if scheduler is not None and 'scheduler' in ckpt:\n",
    "        scheduler.load_state_dict(ckpt['scheduler'])\n",
    "\n",
    "    last_epoch = int(ckpt.get('epoch', 0))\n",
    "    start_epoch = last_epoch + 1\n",
    "\n",
    "    print(f\"  Last epoch: {last_epoch}  →  Start epoch: {start_epoch}\")\n",
    "    return start_epoch\n",
    "\n",
    "\n",
    "def train_joint_epoch(models, loader, optimizer, criterion, raft_transforms, scaler, epoch):\n",
    "    for name, model in models.items():\n",
    "        if name == 'raft':\n",
    "            model.eval()\n",
    "        else:\n",
    "            model.train()\n",
    "\n",
    "    running_total = 0.0\n",
    "    running_rate = 0.0\n",
    "    running_lamdist = 0.0\n",
    "    running_mse = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch} - RTX 5090 Joint Training\", ncols=120)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for batch_idx, (frame_curr, frame_prev, history) in enumerate(pbar):\n",
    "        frame_curr = frame_curr.to(DEVICE, non_blocking=True).clamp(0, 1)\n",
    "        frame_prev = frame_prev.to(DEVICE, non_blocking=True).clamp(0, 1)\n",
    "        history = history.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        # 1. Optical Flow (RAFT)\n",
    "        with torch.no_grad():\n",
    "            img1, img2 = raft_transforms(frame_prev, frame_curr)\n",
    "            flow_gt = models['raft'](img1, img2)[-1]\n",
    "\n",
    "        # 2. Forward + loss\n",
    "        with autocast(\"cuda\", dtype=AUTOCAST_DTYPE):\n",
    "            flow_norm = normalize_flow(flow_gt, MAX_FLOW)\n",
    "\n",
    "            mvae_out = models['mvae'](flow_norm)\n",
    "            flow_refined_norm = models['mrefine'](mvae_out[\"x_hat\"], history)\n",
    "            warped = flow_warp(frame_prev, flow_refined_norm * MAX_FLOW)\n",
    "\n",
    "            residual_gt = frame_curr - warped\n",
    "            rvae_out = models['rvae'](residual_gt)\n",
    "            residual_refined = models['rrefine'](rvae_out[\"x_hat\"])\n",
    "\n",
    "            recon_intermediate = (warped + residual_refined).clamp(0, 1)\n",
    "            mask = compute_adaptive_mask(residual_refined)\n",
    "            final_recon = models['adaptive'](recon_intermediate, warped, mask, history).clamp(0, 1)\n",
    "\n",
    "            loss_rd, dist, rate, mse, _ = criterion(\n",
    "                final_recon,\n",
    "                frame_curr,\n",
    "                [mvae_out.get(\"likelihoods\", None), rvae_out.get(\"likelihoods\", None)]\n",
    "            )\n",
    "\n",
    "            aux = total_aux_loss(models)\n",
    "            loss_total = loss_rd + AUX_WEIGHT * aux\n",
    "            loss_total = loss_total / GRADIENT_ACCUMULATION\n",
    "\n",
    "        scaler.scale(loss_total).backward()\n",
    "\n",
    "        if (batch_idx + 1) % GRADIENT_ACCUMULATION == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                [p for group in optimizer.param_groups for p in group['params']],\n",
    "                max_norm=1.0\n",
    "            )\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        contrib_rate = rate.item()\n",
    "        contrib_lamdist = (criterion.lmbda * dist).item()\n",
    "\n",
    "        running_total += loss_total.item() * GRADIENT_ACCUMULATION\n",
    "        running_rate += contrib_rate\n",
    "        running_lamdist += contrib_lamdist\n",
    "        running_mse += mse.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"L\": f\"{(loss_total.item()*GRADIENT_ACCUMULATION):.4f}\",\n",
    "            \"R\": f\"{contrib_rate:.3f}\",\n",
    "            \"MSE\": f\"{mse.item():.1e}\",\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"loss\": running_total / num_batches,\n",
    "        \"rate\": running_rate / num_batches,\n",
    "        \"lamdist\": running_lamdist / num_batches,\n",
    "        \"mse\": running_mse / num_batches,\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(models, loader, criterion, raft_transforms):\n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "\n",
    "    running_total = 0.0\n",
    "    running_rate = 0.0\n",
    "    running_lamdist = 0.0\n",
    "    running_mse = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"Validation\", ncols=120)\n",
    "\n",
    "    for (frame_curr, frame_prev, history) in pbar:\n",
    "        frame_curr = frame_curr.to(DEVICE, non_blocking=True).clamp(0, 1)\n",
    "        frame_prev = frame_prev.to(DEVICE, non_blocking=True).clamp(0, 1)\n",
    "        history = history.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        img1, img2 = raft_transforms(frame_prev, frame_curr)\n",
    "        flow_gt = models['raft'](img1, img2)[-1]\n",
    "\n",
    "        with autocast(\"cuda\", dtype=AUTOCAST_DTYPE):\n",
    "            flow_norm = normalize_flow(flow_gt, MAX_FLOW)\n",
    "            mvae_out = models['mvae'](flow_norm)\n",
    "            flow_refined_norm = models['mrefine'](mvae_out[\"x_hat\"], history)\n",
    "\n",
    "            warped = flow_warp(frame_prev, flow_refined_norm * MAX_FLOW)\n",
    "            residual_gt = frame_curr - warped\n",
    "            rvae_out = models['rvae'](residual_gt)\n",
    "            residual_refined = models['rrefine'](rvae_out[\"x_hat\"])\n",
    "\n",
    "            recon_intermediate = (warped + residual_refined).clamp(0, 1)\n",
    "            mask = compute_adaptive_mask(residual_refined)\n",
    "            final_recon = models['adaptive'](recon_intermediate, warped, mask, history).clamp(0, 1)\n",
    "\n",
    "            loss_rd, dist, rate, mse, _ = criterion(\n",
    "                final_recon,\n",
    "                frame_curr,\n",
    "                [mvae_out.get(\"likelihoods\", None), rvae_out.get(\"likelihoods\", None)]\n",
    "            )\n",
    "\n",
    "        contrib_rate = rate.item()\n",
    "        contrib_lamdist = (criterion.lmbda * dist).item()\n",
    "\n",
    "        running_total += loss_rd.item()\n",
    "        running_rate += contrib_rate\n",
    "        running_lamdist += contrib_lamdist\n",
    "        running_mse += mse.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"L\": f\"{loss_rd.item():.4f}\",\n",
    "            \"R\": f\"{contrib_rate:.3f}\",\n",
    "            \"MSE\": f\"{mse.item():.1e}\",\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"loss\": running_total / num_batches,\n",
    "        \"rate\": running_rate / num_batches,\n",
    "        \"lamdist\": running_lamdist / num_batches,\n",
    "        \"mse\": running_mse / num_batches,\n",
    "    }\n",
    "\n",
    "\n",
    "def main_joint():\n",
    "    seed_everything(1234)\n",
    "\n",
    "\n",
    "    print(f\"Device: {DEVICE} ({torch.cuda.get_device_name(0)})\")\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"VRAM: {vram_gb:.1f} GB\")\n",
    "\n",
    "    eff_bs = BATCH_SIZE * GRADIENT_ACCUMULATION\n",
    "    print(f\"\\nBatch size (per step): {BATCH_SIZE}\")\n",
    "    print(f\"Gradient Accumulation: {GRADIENT_ACCUMULATION}\")\n",
    "    print(f\" Effective Batch Size: {eff_bs} (Equivalente al config A100)\")\n",
    "\n",
    "    print(f\"Workers: {NUM_WORKERS} / Prefetch: {PREFETCH_FACTOR}\")\n",
    "    print(f\"Precision: {'BFloat16' if USE_BFLOAT16 else 'Float16'}\")\n",
    "    print(f\"Torch Compile: {'ENABLED' if USE_COMPILE else 'DISABLED'} | backend={COMPILE_BACKEND}\")\n",
    "\n",
    "    print(\"\\n Loading models...\")\n",
    "\n",
    "    mvae = ScaleHyperprior(N=192, M=192, in_channels=2).to(DEVICE)\n",
    "    mrefine = MotionRefineNET(base=64, blocks=8).to(DEVICE)\n",
    "    rvae = ScaleHyperprior(N=128, M=128, in_channels=3, out_channels=3).to(DEVICE)\n",
    "    rrefine = ResRefiNET().to(DEVICE)\n",
    "    adaptive = AdaptiveRefiNET(base=64, num_blocks=10).to(DEVICE)\n",
    "\n",
    "    from torchvision.models.optical_flow import raft_small, Raft_Small_Weights\n",
    "    raft = raft_small(weights=Raft_Small_Weights.DEFAULT, progress=False).to(DEVICE)\n",
    "    raft.eval()\n",
    "    for p in raft.parameters():\n",
    "        p.requires_grad = False\n",
    "    raft_transforms = Raft_Small_Weights.DEFAULT.transforms()\n",
    "\n",
    "    models = {\n",
    "        'mvae': mvae,\n",
    "        'mrefine': mrefine,\n",
    "        'rvae': rvae,\n",
    "        'rrefine': rrefine,\n",
    "        'adaptive': adaptive,\n",
    "        'raft': raft\n",
    "    }\n",
    "\n",
    "    lr_scale = math.sqrt(eff_bs / 8.0)\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': mvae.parameters(),     'lr': 1e-6  * lr_scale, 'weight_decay': 1e-4},\n",
    "        {'params': rvae.parameters(),     'lr': 1e-6  * lr_scale, 'weight_decay': 1e-4},\n",
    "        {'params': mrefine.parameters(),  'lr': 5e-6  * lr_scale, 'weight_decay': 1e-4},\n",
    "        {'params': rrefine.parameters(),  'lr': 5e-6  * lr_scale, 'weight_decay': 1e-4},\n",
    "        {'params': adaptive.parameters(), 'lr': 2.5e-5 * lr_scale, 'weight_decay': 1e-4},\n",
    "    ], betas=(0.9, 0.999), eps=1e-8)\n",
    "\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EXTRA_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "    criterion = RateDistortionLossMSE(lmbda=1024.0).to(DEVICE)\n",
    "    scaler = GradScaler(enabled=True)\n",
    "\n",
    "    start_epoch = resume_from_checkpoint(RESUME_PATH, models, optimizer=None, scheduler=None)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    safe_update_entropy(models['mvae'], force=True)\n",
    "    safe_update_entropy(models['rvae'], force=True)\n",
    "\n",
    "    frozen_m = freeze_entropy_modules(models['mvae'])\n",
    "    frozen_r = freeze_entropy_modules(models['rvae'])\n",
    "\n",
    "    if USE_COMPILE:\n",
    "        print(f\"\\n Compiling custom models with torch.compile (backend={COMPILE_BACKEND})...\")\n",
    "        models['mrefine']  = torch.compile(unwrap(models['mrefine']),  backend=COMPILE_BACKEND)\n",
    "        models['rrefine']  = torch.compile(unwrap(models['rrefine']),  backend=COMPILE_BACKEND)\n",
    "        models['adaptive'] = torch.compile(unwrap(models['adaptive']), backend=COMPILE_BACKEND)\n",
    "        print(\" Custom models compiled.\")\n",
    "\n",
    "    print(\"\\n Loading datasets.\")\n",
    "    train_ds = VimeoSeptupletDataset(VIMEO_PARTS, TRAIN_LISTS, transforms.ToTensor())\n",
    "    val_ds = VimeoSeptupletDataset(VIMEO_PARTS, TEST_LISTS, transforms.ToTensor())\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        persistent_workers=PERSISTENT_WORKERS,\n",
    "        prefetch_factor=PREFETCH_FACTOR,\n",
    "        drop_last=True,\n",
    "        multiprocessing_context='fork' if os.name != 'nt' else 'spawn',\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS // 2,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        persistent_workers=PERSISTENT_WORKERS,\n",
    "        prefetch_factor=PREFETCH_FACTOR,\n",
    "        multiprocessing_context='fork' if os.name != 'nt' else 'spawn',\n",
    "    )\n",
    "\n",
    "    end_epoch = start_epoch + EXTRA_EPOCHS - 1\n",
    "    best_val = float('inf')\n",
    "\n",
    "    for epoch in range(start_epoch, end_epoch + 1):\n",
    "        if epoch in LAMBDA_SCHEDULE:\n",
    "            criterion.lmbda = float(LAMBDA_SCHEDULE[epoch])\n",
    "            print(f\"\\n Lambda updated to {criterion.lmbda:.0f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch}/{end_epoch} | λ={criterion.lmbda:.0f} | Eff.Batch={eff_bs}\")\n",
    "\n",
    "        train_metrics = train_joint_epoch(\n",
    "            models, train_loader, optimizer, criterion,\n",
    "            raft_transforms, scaler, epoch\n",
    "        )\n",
    "        print(f\"\\n Epoch {epoch} (TRAIN): Loss={train_metrics['loss']:.6f} | MSE={train_metrics['mse']:.6e}\")\n",
    "\n",
    "        # Sync + update entropy (eager)\n",
    "        torch.cuda.synchronize()\n",
    "        safe_update_entropy(models['mvae'], force=True)\n",
    "        safe_update_entropy(models['rvae'], force=True)\n",
    "\n",
    "        val_metrics = validate(models, val_loader, criterion, raft_transforms)\n",
    "        print(f\" Epoch {epoch} (VAL): Loss={val_metrics['loss']:.6f} | MSE={val_metrics['mse']:.6e}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        checkpoint_path = os.path.join(OUTPUT_DIR, f\"joint_epoch_{epoch}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'mvae': safe_state_dict(models['mvae']),\n",
    "            'mrefine': safe_state_dict(models['mrefine']),\n",
    "            'rvae': safe_state_dict(models['rvae']),\n",
    "            'rrefine': safe_state_dict(models['rrefine']),\n",
    "            'adaptive': safe_state_dict(models['adaptive']),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "        }, checkpoint_path)\n",
    "\n",
    "        if val_metrics['loss'] < best_val:\n",
    "            best_val = val_metrics['loss']\n",
    "            best_path = os.path.join(OUTPUT_DIR, \"joint_best.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'mvae': safe_state_dict(models['mvae']),\n",
    "                'mrefine': safe_state_dict(models['mrefine']),\n",
    "                'rvae': safe_state_dict(models['rvae']),\n",
    "                'rrefine': safe_state_dict(models['rrefine']),\n",
    "                'adaptive': safe_state_dict(models['adaptive']),\n",
    "                'val_metrics': val_metrics,\n",
    "            }, best_path)\n",
    "            print(f\" model saved Val loss: {best_val:.6f}\")\n",
    "\n",
    "    print(\"\\n finish !\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_joint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da42e66-0def-4e06-9ff2-41dfdb6031c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
